{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Strategy\n",
    "\n",
    "## Introduction to Machine Learning Strategy\n",
    "\n",
    "### Why ML Strategy?\n",
    "- Introduction\n",
    "  - Welcome to the course on how to structure your machine learning project\n",
    "  - Objective: Learn to quickly and efficiently make machine learning systems work\n",
    "- Understanding Machine Learning Strategy\n",
    "  - Motivating example: Working on a cat classification system\n",
    "  - Current system accuracy: 90%, not sufficient for the application\n",
    "  - Various ideas to improve the system:\n",
    "    - Collect more training data\n",
    "    - Increase diversity in training set (cat images with different poses, diverse negative examples)\n",
    "    - Train the algorithm longer with gradient descent\n",
    "    - Try different optimization algorithms (e.g., Adam optimization)\n",
    "    - Experiment with network size (bigger/smaller), dropout, L2 regularization\n",
    "    - Modify network architecture (activation functions, hidden unit count, etc.)\n",
    "- Challenges in Improving Deep Learning Systems\n",
    "  - Abundance of ideas to try\n",
    "  - Risk of wasting time on ineffective approaches\n",
    "  - Example: Spending six months collecting more data with minimal improvement\n",
    "- Importance of Effective Strategies\n",
    "  - Limited time availability for problem-solving\n",
    "  - Need for quick and reliable ways to identify promising ideas\n",
    "- Course Objectives\n",
    "  - Teach strategies to analyze machine learning problems effectively\n",
    "  - Share lessons learned from building and shipping deep learning products\n",
    "  - Unique insights not commonly taught in university deep learning courses\n",
    "- Evolution of Machine Learning Strategy in the Deep Learning Era\n",
    "  - Deep learning algorithms offer new possibilities\n",
    "  - Strategies differ from previous generation machine learning algorithms\n",
    "- Conclusion\n",
    "  - Application of the course's ideas to improve deep learning systems\n",
    "  - Goal: Enhance effectiveness in achieving successful outcomes\n",
    "\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonalization\n",
    "\n",
    "Orthogonalization - TV Example\n",
    "- Old school televisions had multiple knobs to adjust various aspects of the picture.\n",
    "- The TV designers ensured that each knob had a specific function, making it easier to adjust the picture.\n",
    "- Orthogonalization refers to designing knobs that perform distinct functions, allowing precise tuning of the TV image.\n",
    "\n",
    "Orthogonalization - Car Example\n",
    "- Cars have separate controls for steering, acceleration, and braking, making it easy to understand the effects of each action.\n",
    "- If controls were combined, it would be harder to achieve desired steering and speed.\n",
    "- Orthogonal controls aligned with specific actions make it easier to tune the car.\n",
    "\n",
    "Orthogonalization in Machine Learning\n",
    "- To achieve good performance in supervised learning, four criteria must be met: training set performance, dev set performance, test set performance, and real-world performance.\n",
    "- Each criterion requires specific tuning knobs to address potential issues.\n",
    "- For example, a bigger network or better optimization algorithm can be used to improve training set performance.\n",
    "- Regularization techniques can be applied to enhance dev set performance.\n",
    "- Adjusting the size of the dev set can help if the algorithm performs well on the dev set but not the test set.\n",
    "- If the algorithm performs well on the test set but not in the real world, the dev set or cost function may need to be modified.\n",
    "\n",
    "Orthogonalization and Knobs in Machine Learning\n",
    "- Orthogonalized controls allow for clear identification and adjustment of specific issues in a machine learning system.\n",
    "- Early stopping, although widely used, affects both training set and dev set performance simultaneously, making it less orthogonalized.\n",
    "- Using more orthogonalized controls simplifies the tuning process of neural networks.\n",
    "- Orthogonalization helps diagnose performance bottlenecks and identify the specific set of knobs to improve system performance.\n",
    "\n",
    "Diagnosing Performance Bottlenecks and Tuning\n",
    "- The process involves identifying the limitations of the machine learning system's performance.\n",
    "- Understanding which aspect (training set, dev set, test set, or real-world performance) needs improvement.\n",
    "- Determining the specific set of knobs or adjustments to address the identified problem.\n",
    "- Detailed explanation of this process will be provided in the following week's discussions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up your Goal\n",
    "\n",
    "### Single Number Evaluation Metric\n",
    "\n",
    "Introduction - Importance of a Single Real Number Evaluation Metric\n",
    "- Tuning hyperparameters, trying out different learning algorithms, and exploring various options for building machine learning systems are common tasks.\n",
    "- Progress is faster when there is a single real number evaluation metric that quickly indicates if a new approach is better or worse than the previous one.\n",
    "- Setting up a single real number evaluation metric is recommended at the beginning of a machine learning project.\n",
    "- Example: Evaluating classifiers using precision and recall.\n",
    "\n",
    "Precision and Recall\n",
    "- Precision: Percentage of classifier-recognized examples that are actually cats.\n",
    "- Recall: Percentage of actual cat images correctly recognized by the classifier.\n",
    "- Tradeoff exists between precision and recall, and both metrics are important.\n",
    "- Precision and recall alone make it difficult to determine the better classifier.\n",
    "\n",
    "Combining Precision and Recall - F1 Score\n",
    "- F1 score is a standard way to combine precision and recall.\n",
    "- The F1 score is the harmonic mean of precision and recall.\n",
    "- F1 score is defined as 2/ (1/P + 1/R).\n",
    "- Classifier A has a better F1 score in the example, making it the preferable choice.\n",
    "\n",
    "Benefits of a Single Number Evaluation Metric\n",
    "- Having a well-defined dev set and a single number evaluation metric accelerates the iteration process.\n",
    "- A single number evaluation metric helps quickly determine the superior classifier (e.g., classifier A or B).\n",
    "- It improves the efficiency of improving machine learning algorithms.\n",
    "\n",
    "Evaluating Performance in Different Geographies\n",
    "- Scenario: Building a cat app for cat lovers in four major geographies (US, China, India, Rest of the World).\n",
    "- Classifiers achieve different errors in each geography.\n",
    "- Tracking four numbers for each geography makes it difficult to determine the superior algorithm.\n",
    "- Computing the average performance provides a single real number evaluation metric.\n",
    "- Algorithm C has the lowest average error, making it a potential choice for further iteration.\n",
    "\n",
    "Conclusion\n",
    "- A single number evaluation metric enhances decision-making efficiency in machine learning.\n",
    "- It helps determine if an idea or approach is effective.\n",
    "- The discussion will continue in the next video, focusing on setting up optimizing and satisfying metrics.\n",
    "\n",
    "Setting Up Optimizing and Satisfying Metrics\n",
    "- The next video will cover how to effectively set up optimizing and satisfying metrics.\n",
    "- Optimizing metrics focus on maximizing a specific performance measure (e.g., accuracy, F1 score).\n",
    "- Satisfying metrics prioritize meeting certain criteria or thresholds (e.g., error rate below a specified value).\n",
    "- Choosing the appropriate metric depends on the problem and project requirements.\n",
    "\n",
    "Importance of Clear Evaluation Metrics\n",
    "- Clear evaluation metrics provide guidance and clarity in decision-making.\n",
    "- They enable teams to assess the effectiveness of different approaches objectively.\n",
    "- Well-defined evaluation metrics facilitate communication and alignment among team members.\n",
    "\n",
    "Iterative Process in Machine Learning\n",
    "- The workflow in machine learning often involves generating ideas, implementing them, and evaluating their impact.\n",
    "- A single number evaluation metric helps track the progress and effectiveness of each iteration.\n",
    "- Iterating based on the evaluation metric improves the algorithm and enhances overall performance.\n",
    "\n",
    "Summary\n",
    "- Having a single number evaluation metric is crucial for efficiently evaluating machine learning models.\n",
    "- Precision and recall are common metrics, but a tradeoff exists between them.\n",
    "- The F1 score combines precision and recall, providing a balanced evaluation.\n",
    "- Computing averages can help compare performance in different categories or geographies.\n",
    "- Optimizing and satisfying metrics play a role in measuring specific goals and criteria.\n",
    "- Clear evaluation metrics improve decision-making and facilitate communication within teams."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satisficing and Optimiznig Metric\n",
    "\n",
    "Combining Optimizing and Satisficing Metrics\n",
    "- In some cases, it's challenging to combine multiple evaluation metrics into a single row number.\n",
    "- Setting up both optimizing and satisficing metrics can be useful to address this challenge.\n",
    "- Example: Suppose you care about the classification accuracy and running time of a cat classifier.\n",
    "- Combining accuracy and running time using a linear weighted sum may seem artificial.\n",
    "- An alternative approach is to choose a classifier that maximizes accuracy while keeping the running time below a specified threshold.\n",
    "- Accuracy is an optimizing metric, as you aim to maximize its value.\n",
    "- Running time becomes a satisficing metric, where it only needs to be good enough (e.g., less than 100 milliseconds).\n",
    "- Defining both metrics allows for a trade-off between accuracy and running time.\n",
    "- Users may not significantly differentiate between running times below the threshold.\n",
    "- The approach enables the selection of the \"best classifier\" based on the criteria.\n",
    "- In general, when there are N metrics, it's reasonable to choose one as optimizing and the remaining N-1 as satisficing.\n",
    "- The satisficing metrics need to reach a specific threshold but don't require further improvement.\n",
    "\n",
    "Example: Wake Word Detection System\n",
    "- Building a wake word detection system (e.g., \"Alexa,\" \"Hey Siri\") involves accuracy and false positives.\n",
    "- Maximizing accuracy when recognizing trigger words is crucial.\n",
    "- Setting a threshold for false positives per 24 hours (e.g., at most one) becomes the satisficing metric.\n",
    "- By combining these metrics, the system aims to achieve high accuracy while limiting false positives.\n",
    "\n",
    "Summary of Combining Metrics\n",
    "- Combining optimizing and satisficing metrics allows for a comprehensive evaluation.\n",
    "- Choose one metric as optimizing to maximize its performance.\n",
    "- The remaining metrics become satisficing, with thresholds that need to be met.\n",
    "- This approach facilitates comparing and selecting the best option based on the combined metrics.\n",
    "\n",
    "Training, Development, and Test Sets\n",
    "- Evaluation metrics are calculated using training, development (dev), or test sets.\n",
    "- Proper setup of these sets is crucial for reliable performance evaluation.\n",
    "- The next video will provide guidelines on how to set up training, dev, and test sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Dev/Test Distributions\n",
    "\n",
    "Importance of Setting up Training Dev and Test Sets\n",
    "- Proper setup of training, dev (development), and test sets significantly impacts the progress and efficiency of machine learning teams.\n",
    "- Poorly designed data sets can hinder progress instead of facilitating it.\n",
    "- The focus in this video is on setting up dev and test sets.\n",
    "\n",
    "Dev Set and Its Role in Machine Learning Workflow\n",
    "- The dev set, also known as the development set or hold-out cross-validation set, plays a crucial role in the machine learning workflow.\n",
    "- Machine learning teams generate various ideas and train different models on the training set.\n",
    "- The dev set is then used to evaluate these ideas and select the most promising one.\n",
    "- Continuous innovation and experimentation are conducted to improve the performance of the selected model on the dev set.\n",
    "- The goal is to achieve satisfactory results on the dev set before evaluating the model on the test set.\n",
    "\n",
    "Setting up Dev and Test Sets for a Cat Classifier Example\n",
    "- Suppose you are building a cat classifier for different regions, such as the U.S., U.K., Europe, South America, India, China, other Asian countries, and Australia.\n",
    "- Setting up dev and test sets in a specific way is crucial.\n",
    "- In the example provided, four regions are randomly chosen for the dev set, while the other four regions are chosen for the test set.\n",
    "- However, this approach is flawed because the dev and test sets come from different distributions.\n",
    "\n",
    "Importance of Dev and Test Sets Having the Same Distribution\n",
    "- It is recommended to ensure that the dev and test sets have the same distribution.\n",
    "- Treating the dev set as a target allows the team to quickly innovate, experiment, and evaluate different models.\n",
    "- Teams excel at aiming for a target, making improvements, and getting closer to hitting the bullseye on the dev set.\n",
    "- Having dev and test sets from different distributions can lead to unexpected performance discrepancies.\n",
    "- Months of optimization based on the dev set might not translate well when evaluated on the test set.\n",
    "- It is frustrating for the team to realize that the target has been shifted to a different location after significant efforts.\n",
    "- To avoid this, include randomly shuffled data from all regions in both the dev and test sets.\n",
    "- This ensures that both sets come from the same distribution, representing the mixed data of all regions.\n",
    "\n",
    "Example of Mismatched Dev and Test Sets\n",
    "- A true story example involves a machine learning team optimizing a model for loan approvals in medium income zip codes.\n",
    "- After several months of work, they decided to test the model on data from low income zip codes.\n",
    "- The distribution of medium and low income zip codes is significantly different, leading to poor performance on the test set.\n",
    "- The team wasted three months of work and had to redo substantial portions.\n",
    "- The team aimed for one target for three months and then faced frustration when asked to hit a different target.\n",
    "\n",
    "Recommendations for Dev and Test Set Setup\n",
    "- Choose a dev and test set that reflects the data expected in the future and is important for good performance.\n",
    "- Ensure that both sets come from the same distribution to accurately represent the desired target.\n",
    "- Aligning the sets with future data expectations allows the team to aim at the desired target efficiently.\n",
    "- The training set setup will be discussed in a separate video.\n",
    "- Setting up the dev set and evaluation metric defines the target for the machine learning team.\n",
    "- The size of the dev and test sets, along with considerations in the era of deep learning, will be discussed in the next video."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of the Dev and Test Sets\n",
    "\n",
    "Setting Up Dev and Test Sets in the Deep Learning Era\n",
    "\n",
    "- Historical Guidelines for Dev and Test Sets\n",
    "  - Traditional rule of thumb: 70/30 or 60/20/20 split for train and test sets\n",
    "  - Reasonable when dealing with smaller dataset sizes\n",
    "    - E.g., 100 examples: 70/30 or 60/20/20 split\n",
    "    - E.g., 1,000 examples or 10,000 examples: similar splits still reasonable\n",
    "\n",
    "- Changes in the Modern Machine Learning Era\n",
    "  - Working with larger dataset sizes\n",
    "  - Example: Having a million training examples\n",
    "    - 98% for training set, 1% for dev set, 1% for test set\n",
    "    - Dev and test sets can be smaller due to the abundance of training data\n",
    "\n",
    "- Test Set Size Considerations\n",
    "  - Purpose of the test set: Evaluate the final system's performance\n",
    "  - Set the test set size to provide high confidence in overall system performance\n",
    "  - Large test sets not always necessary\n",
    "    - Confidence gained from 10,000 examples may suffice for application-specific performance evaluation\n",
    "  - Test set size depends on the available data\n",
    "    - May be much less than 30% of the overall dataset\n",
    "\n",
    "- Train and Dev Sets without a Test Set\n",
    "  - Some applications may not require high confidence in the overall system performance\n",
    "  - Train and dev sets can be used, omitting the test set\n",
    "  - Referred to as the \"train dev set\"\n",
    "  - Dev set is used for tuning, acting as a substitute for the test set\n",
    "  - Not recommended to omit the test set, but it can be acceptable in specific cases\n",
    "\n",
    "- Summary of Setting Up Dev and Test Sets\n",
    "  - Old rule of thumb (70/30 split) no longer applies in the era of big data\n",
    "  - More data allocated for training, less for dev and test sets, especially with large datasets\n",
    "  - Dev set should be set sufficiently large for its purpose, allowing evaluation and idea comparison\n",
    "  - Test set should be sized adequately for evaluating the final model's performance\n",
    "    - Can be much smaller than 30% of the data\n",
    "  - Guidelines provided for setting up dev and test sets in the Deep Learning era\n",
    "\n",
    "Next: Changing Evaluation Metric or Dev and Test Sets Midway in a Machine Learning Problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Change Dev/Test Sets and Metrics?\n",
    "\n",
    "- Example of Misleading Evaluation Metric\n",
    "  - Building a cat classifier for cat-loving users\n",
    "  - Initial metric: Classification error\n",
    "  - Algorithm A: 3% error, Algorithm B: 5% error\n",
    "  - Algorithm A allows pornographic images, Algorithm B doesn't\n",
    "  - Algorithm B considered better due to the absence of pornographic images, despite higher error rate\n",
    "\n",
    "- Significance of Evaluation Metric and Dev Set\n",
    "  - Evaluation metric acts as a target for the team to aim at\n",
    "  - Dev set helps rank algorithms based on the metric\n",
    "  - In the example, the evaluation metric misrepresents Algorithm A as better\n",
    "\n",
    "- Need for Metric and Set Adjustment\n",
    "  - When evaluation metric no longer accurately ranks algorithm preferences\n",
    "  - Consider changing the evaluation metric, dev set, or test set\n",
    "\n",
    "- Modifying the Evaluation Metric\n",
    "  - Issue with the misclassification error metric in the example\n",
    "  - Pornographic and non-pornographic images treated equally\n",
    "  - Desire to avoid mislabeling pornographic images as cat images\n",
    "  - Suggestion: Introduce weight term (w(i)) to differentiate between pornographic and non-pornographic images\n",
    "    - Higher weight for pornographic images (e.g., 10 times)\n",
    "  - Implementation involves labeling pornographic images in the dev and test sets\n",
    "\n",
    "- Importance of Defining a New Evaluation Metric\n",
    "  - If the current metric fails to rank algorithms correctly\n",
    "  - Goal: Accurately determine the better algorithm for the application\n",
    "  - Different ways to define a new evaluation metric\n",
    "  - Don't hesitate to redefine the metric if unsatisfied with the current one\n",
    "\n",
    "- Expanding the Scope of Evaluation Metrics\n",
    "  - Focus on defining metrics for evaluating classifiers\n",
    "  - Metrics should reflect preferences and goals of selecting a better algorithm\n",
    "  - Not limited to the example of detecting pornographic images\n",
    "\n",
    "- Breaking Down the Problem\n",
    "  - Example of orthogonalization in machine learning\n",
    "  - Divide the machine learning task into distinct steps\n",
    "  - Step 1: Define the metric that captures the objective\n",
    "  - Step 2: Focus on achieving high performance on the defined metric\n",
    "\n",
    "- Placing the Target\n",
    "  - Analogy of placing the target in target shooting\n",
    "  - Separate step from aiming and shooting\n",
    "  - Placing the target is analogous to defining the metric\n",
    "  - Consider it as a separate knob to tune for algorithm performance\n",
    "\n",
    "- Modifying the Cost Function\n",
    "  - Adjusting the cost function in the learning algorithm\n",
    "  - Incorporating weights to differentiate between different examples\n",
    "  - Modifying the normalization constant for the cost function\n",
    "\n",
    "- Importance of Orthogonalization\n",
    "  - Philosophy of separating the steps of placing the target and shooting\n",
    "  - Define the metric first, then optimize for it\n",
    "  - Encouragement to think of defining the metric as one step\n",
    "  - Modify the approach (e.g., cost function) to excel at the defined metric\n",
    "\n",
    "- Example of Metric and Dev/Test Set Mismatch\n",
    "  - Scenario: Two cat classifiers, A and B\n",
    "  - Dev set evaluation: A has 3% error, B has 5% error\n",
    "  - Deployment scenario differs from the evaluation set\n",
    "  - Users upload lower quality, less well-framed images\n",
    "  - Algorithm B performs better in the deployment scenario\n",
    "\n",
    "- Adjusting Metric and Dev/Test Set\n",
    "  - If the current metric and dev/test set don't reflect the desired application performance\n",
    "  - Evaluation on high-quality images doesn't predict real-world performance\n",
    "  - Change the metric and/or the dev/test set to match the application needs\n",
    "\n",
    "- Speeding Up Iteration with Metrics and Dev Set\n",
    "  - Benefits of having an evaluation metric and dev set\n",
    "  - Enables faster decision-making and iteration\n",
    "  - Set up a preliminary metric and dev set quickly\n",
    "  - Continuously improve and refine them over time\n",
    "\n",
    "- Recommendation and Efficiency\n",
    "  - Suggestion to set up an evaluation metric and dev set early on\n",
    "  - Iteration efficiency and team performance are improved\n",
    "  - It's acceptable to change the metric and dev/test set later if needed\n",
    "  - Discouragement from running without any evaluation metric or dev set\n",
    "\n",
    "- Conclusion\n",
    "  - Guidelines for changing the evaluation metric and dev/test sets\n",
    "  - Setting up a well-defined target for efficient iteration and performance improvement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Human-level Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Human-Level Performance?\n",
    "\n",
    "- Comparing Machine Learning to Human-Level Performance\n",
    "  - Recently, there has been a growing interest in comparing machine learning systems to human-level performance.\n",
    "  - Two main reasons for this trend:\n",
    "    - Advances in deep learning have significantly improved the performance of machine learning algorithms, making them competitive with humans in many application areas.\n",
    "    - Designing and building machine learning systems becomes more efficient when targeting tasks that humans can perform.\n",
    "\n",
    "- Progress Towards Human-Level Performance\n",
    "  - Progress in machine learning tasks over time:\n",
    "    - Rapid improvement as the algorithm approaches human-level performance.\n",
    "    - After surpassing human-level performance, progress and accuracy tend to slow down.\n",
    "    - The hope is to achieve some theoretical optimum level of performance.\n",
    "\n",
    "- Bayes Optimal Error\n",
    "  - Bayes optimal error is the best possible error in mapping from input (x) to output (y).\n",
    "  - The perfect level of accuracy may not be 100% due to factors like noise in audio or blurriness in images.\n",
    "  - Bayes optimal error represents the theoretical best performance that cannot be surpassed.\n",
    "\n",
    "- Reasons for Slowing Down After Surpassing Human-Level Performance\n",
    "  - Human-level performance is often not far from Bayes optimal error, leaving less room for improvement.\n",
    "  - Certain tools for performance improvement, such as using labeled data from humans and manual error analysis, are more effective when the algorithm is worse than humans.\n",
    "  - Once the algorithm surpasses human-level performance, these tactics become harder to apply.\n",
    "\n",
    "- Importance of Comparing to Human-Level Performance\n",
    "  - Comparing to human-level performance helps in tasks that humans excel at.\n",
    "  - Humans can provide labeled data and insights into algorithmic errors, improving the algorithm's performance.\n",
    "  - Comparing to human performance aids in analyzing bias and variance.\n",
    "  - Understanding human capabilities helps determine the focus on reducing bias and variance in machine learning algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoidable Bias\n",
    "\n",
    "- The Importance of Human Level Performance\n",
    "  - Knowing human level performance helps determine the desired level of performance for the learning algorithm on the training set.\n",
    "  - It guides the decision of how well, but not too well, the algorithm should perform on the training set.\n",
    "\n",
    "- Influence of Human Level Error on Bias and Variance\n",
    "  - The comparison of algorithm performance to human level performance reveals insights about bias and variance.\n",
    "  - Scenario 1: Human level error is 1%\n",
    "    - If the algorithm achieves 8% training error and 10% dev error, it indicates a significant gap between algorithm and human performance.\n",
    "    - Focus on reducing bias by training a bigger neural network or running the training set longer to improve performance on the training set.\n",
    "  - Scenario 2: Human level error is 7.5%\n",
    "    - Even with the same training error and dev error as in the previous scenario, the algorithm is considered to perform well.\n",
    "    - Focus on reducing variance by employing regularization or acquiring more training data to reduce the gap between training error and dev error.\n",
    "\n",
    "- Human Level Error as a Proxy for Bayes Error\n",
    "  - Human level error serves as an estimate or proxy for Bayes error, which represents the best achievable performance.\n",
    "  - Human level error is typically close to Bayes error in computer vision tasks where humans excel.\n",
    "  - The difference between human level error and training error is termed \"avoidable bias,\" representing the minimum level of error that cannot be surpassed without overfitting.\n",
    "  - The difference between training error and dev error is an indicator of the algorithm's variance problem.\n",
    "\n",
    "- Tailoring Tactics Based on Human Level Performance\n",
    "  - Understanding human level error helps determine the appropriate tactics to focus on.\n",
    "  - High human level error indicates potential for bias reduction tactics.\n",
    "  - Low human level error suggests a need for variance reduction tactics.\n",
    "  - The avoidable bias and variance measure the potential for improvement in the algorithm's performance.\n",
    "\n",
    "- Considerations for Decision Making\n",
    "  - Human level performance has a nuanced impact on decision making and focus areas.\n",
    "  - Factors like understanding the estimate of Bayes error and the level of avoidable bias influence the choice of tactics.\n",
    "  - Different scenarios require different approaches to improve the algorithm's performance.\n",
    "\n",
    "This video emphasizes the significance of human level performance in guiding the optimization of machine learning algorithms. By comparing algorithm performance to human level performance, decisions regarding bias and variance reduction tactics can be tailored accordingly. Understanding the relationship between human level error, avoidable bias, and variance helps in optimizing the algorithm's performance on training and development sets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Human-level Performance\n",
    "\n",
    "- Definition of Human-Level Performance\n",
    "  - The term \"human-level performance\" is often used loosely in research articles.\n",
    "  - It is important to define it more precisely for driving progress in machine learning projects.\n",
    "  - The definition that is most useful is the one related to estimating Bayes error.\n",
    "\n",
    "- Estimating Bayes Error\n",
    "  - Human-level error is used as a way to estimate Bayes error.\n",
    "  - Bayes error represents the best possible error any function could achieve.\n",
    "  - Example: Medical image classification\n",
    "    - Different levels of human performance: untrained human (3% error), typical doctor (1% error), experienced doctor (0.7% error), team of experienced doctors (0.5% error).\n",
    "    - Defining human-level error: It is a proxy or estimate for Bayes error.\n",
    "    - Bayes error is less than or equal to the best achievable human performance (0.5% or lower).\n",
    "\n",
    "- Different Definitions of Human-Level Error\n",
    "  - Purpose-driven definitions: For research papers or system deployment.\n",
    "    - Definition 1: Surpassing a typical doctor's performance.\n",
    "    - Definition 2: Surpassing a single radiologist doctor's performance.\n",
    "\n",
    "- Importance of Clear Definition\n",
    "  - Define human-level error based on the intended purpose.\n",
    "  - If the goal is to estimate Bayes error, use the performance achieved by a team of human doctors.\n",
    "  - An error analysis example: Training error (5%), dev error (6%).\n",
    "    - Measure of avoidable bias: The gap between Bayes error and training error.\n",
    "    - Measure of variance problem: The difference between Bayes error and dev error.\n",
    "\n",
    "- Impact of Definitions on Bias and Variance\n",
    "  - Example 1: Training error (5%), dev error (6%)\n",
    "    - Measure of avoidable bias: Around 4% (depending on the definition of human-level error).\n",
    "    - Bias reduction techniques should be the focus.\n",
    "  - Example 2: Training error (1%), dev error (5%)\n",
    "    - Measure of avoidable bias: Around 0% to 0.5% (depending on the definition of human-level error).\n",
    "    - Variance reduction techniques should be the focus.\n",
    "  - Example 3: Training error (0.7%), dev error (0.8%)\n",
    "    - Measure of avoidable bias: 0.2%.\n",
    "    - Measure of variance problem: 0.1%.\n",
    "    - Both bias and variance need attention.\n",
    "\n",
    "- Difficulty of Progress at Human-Level Performance\n",
    "  - Teasing out bias and variance effects becomes harder.\n",
    "  - Estimating Bayes error accurately is crucial.\n",
    "  - Progress in machine learning projects becomes more challenging.\n",
    "\n",
    "- Importance of Human-Level Performance Estimate\n",
    "  - Human-level error serves as a proxy or approximation for Bayes error.\n",
    "  - Difference between Bayes error estimate and training error indicates avoidable bias.\n",
    "  - Difference between training error and dev error indicates variance.\n",
    "\n",
    "- Nuanced Analysis for Non-Zero Bayes Error\n",
    "  - In some cases, Bayes error is non-zero and can't be expected to reach 0%.\n",
    "  - Previous analysis using training error compared to 0% is insufficient.\n",
    "  - Better estimates for Bayes error help understand avoidable bias and variance.\n",
    "\n",
    "- Advantages of Human-Level Performance Estimate\n",
    "  - Helps make decisions on bias reduction or variance reduction tactics.\n",
    "  - Works well until surpassing human-level performance.\n",
    "  - Estimating Bayes error accurately becomes challenging beyond human-level performance.\n",
    "\n",
    "- Surpassing Human-Level Performance\n",
    "  - Deep learning has enabled surpassing human-level performance in many tasks.\n",
    "  - Further discussion on the process of surpassing human-level performance in the next video."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Surpassing Human-level Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Excitement in Surpassing Human-Level Performance:\n",
    "  - Many teams find it thrilling to exceed human-level performance in a specific recreational classification task.\n",
    "  - The difficulty of machine learning progress increases as it approaches or surpasses human-level performance.\n",
    "\n",
    "- Evaluating Avoidable Bias and Variance - Example 1:\n",
    "  - Problem scenario: A team of humans achieves 0.5% error, a single human has 1% error, and the algorithm has 0.6% training error and 0.8% dev error.\n",
    "  - Estimate of Bayes' error: 0.5%.\n",
    "  - Avoidable bias calculation: The difference between human-level error (0.5%) and the algorithm's training error (0.6%) suggests an avoidable bias of at least 0.1%.\n",
    "  - Variance calculation: The difference between training error (0.6%) and dev error (0.8%) indicates a variance of approximately 0.2%.\n",
    "  - Possible focus on reducing variance over avoidable bias in this case.\n",
    "\n",
    "- Challenges in Evaluating Avoidable Bias - Example 2:\n",
    "  - Problem scenario: A team of humans and a single human perform the same as before, but the algorithm has 0.3% training error and 0.4% dev error.\n",
    "  - It becomes more difficult to determine the avoidable bias in this scenario.\n",
    "  - Uncertainty arises regarding whether the training error (0.3%) indicates overfitting by 0.2%, or if Bayes' error is actually 0.1%, 0.2%, or 0.3%.\n",
    "  - Insufficient information is available to decide whether to focus on reducing bias or variance in the algorithm.\n",
    "  - Lack of clarity hinders progress efficiency.\n",
    "\n",
    "- Implications of Surpassing Human-Level Performance:\n",
    "  - Surpassing the 0.5% threshold makes it challenging to rely on human intuition for further algorithm improvement.\n",
    "  - Machine learning significantly surpasses human-level performance in various areas, including online advertising, product recommendations, logistics, and loan prediction.\n",
    "  - Notable observation: These examples involve structured data rather than natural perception tasks like computer vision or speech recognition.\n",
    "  - Humans excel in natural perception tasks, making it harder for computers to surpass human-level performance in those areas.\n",
    "\n",
    "- Role of Data Availability in Surpassing Human-Level Performance:\n",
    "  - Problems where machine learning excels typically involve teams with access to extensive data.\n",
    "  - Examples include online advertising, product recommendations, logistics, and loan prediction.\n",
    "  - Deep learning systems benefit from examining large amounts of data to discover statistical patterns beyond the capabilities of the human mind.\n",
    "\n",
    "- Surpassing Human-Level Performance in Speech Recognition and Computer Vision:\n",
    "  - Speech recognition systems have achieved performance surpassing humans.\n",
    "  - Some computer vision and image recognition tasks have also seen computers surpassing human-level performance.\n",
    "  - Natural perception tasks are challenging for computers due to humans' inherent strength in these areas.\n",
    "\n",
    "- Advances in Medical Tasks:\n",
    "  - Certain medical tasks, such as reading ECGs, diagnosing skin cancer, and specific radiology tasks, show computers achieving performance beyond a single human.\n",
    "  - Recent developments in deep learning have enabled surpassing human-level performance in some medical applications, despite the difficulty posed by natural perception tasks.\n",
    "\n",
    "- Surpassing Human-Level Performance with Sufficient Data:\n",
    "  - Surpassing human-level performance is challenging but achievable through deep learning and the availability of ample data.\n",
    "  - Deep learning systems have achieved human-level performance and beyond in numerous applications.\n",
    "  - Success relies on training deep learning systems with enough data for a specific supervisory problem.\n",
    "\n",
    "- Encouragement for Future Deep Learning System Success:\n",
    "  - The text expresses hope that deep learning systems, with continued advancements, will eventually surpass human-level performance.\n",
    "  - It acknowledges that surpassing human-level performance is not an easy task but emphasizes the potential for significant progress.\n",
    "  - The examples provided throughout the text serve as inspiration and motivation for individuals working on deep learning systems to strive for surpassing human-level performance in their respective applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving your Model Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Fundamental Assumptions for a Supervised Learning Algorithm:\n",
    "    - The algorithm can fit the training set well, indicating low avoidable bias.\n",
    "    - Performance on the training set generalizes effectively to the dev set or test set, implying manageable variance.\n",
    "- Orthogonalization Approach:\n",
    "    - Separate knobs for addressing avoidable bias issues and variance problems.\n",
    "- Evaluating Performance Improvement:\n",
    "    - Calculate the difference between training error and proxy for Bayes error to estimate avoidable bias.\n",
    "    - Determine the difference between dev error and training error as an indication of the magnitude of the variance problem.\n",
    "- Reducing Avoidable Bias:\n",
    "    - Tactics for reducing avoidable bias:\n",
    "        - Training a larger model to achieve better performance on the training set.\n",
    "        - Extending training duration.\n",
    "        - Utilizing advanced optimization algorithms (e.g., ADS momentum, RMSprop, Adam).\n",
    "        - Exploring alternative neural network architectures and hyperparameters.\n",
    "- Addressing Variance Problems:\n",
    "    - Techniques to mitigate variance problems:\n",
    "        - Acquiring more training data to enhance generalization to unseen dev set data.\n",
    "        - Employing regularization methods such as L2 regularization, dropout, and data augmentation.\n",
    "        - Conducting neural network architecture and hyperparameter search for better model fit.\n",
    "- Mastery of Bias and Variance:\n",
    "    - Understanding the concepts of bias and variance is crucial but challenging to master.\n",
    "    - Systematically applying the learned concepts enables more efficient and strategic improvement of machine learning system performance.\n",
    "- Homework and Conclusion:\n",
    "    - Homework assignment allows practice and application of the discussed concepts.\n",
    "    - Wishes good luck with the assignment and anticipation of upcoming videos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
